import tt_lib.device
from _typeshed import Incomplete
from typing import Any, ClassVar, Iterator, List, Tuple, overload

class BcastOpDim:
    __members__: ClassVar[dict] = ...  # read-only
    H: ClassVar[BcastOpDim] = ...
    HW: ClassVar[BcastOpDim] = ...
    W: ClassVar[BcastOpDim] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class BcastOpMath:
    __members__: ClassVar[dict] = ...  # read-only
    ADD: ClassVar[BcastOpMath] = ...
    MUL: ClassVar[BcastOpMath] = ...
    SUB: ClassVar[BcastOpMath] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class BufferType:
    __members__: ClassVar[dict] = ...  # read-only
    DRAM: ClassVar[BufferType] = ...
    L1: ClassVar[BufferType] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class CoreCoord:
    @overload
    def __init__(self, arg0: int, arg1: int) -> None: ...
    @overload
    def __init__(self, arg0: Tuple[int, int]) -> None: ...
    @property
    def x(self) -> int: ...
    @property
    def y(self) -> int: ...

class CoreRange:
    def __init__(self, arg0: CoreCoord, arg1: CoreCoord) -> None: ...

class CoreRangeSet:
    def __init__(self, arg0: Set[CoreRange]) -> None: ...

class DataType:
    __members__: ClassVar[dict] = ...  # read-only
    BFLOAT16: ClassVar[DataType] = ...
    BFLOAT8_B: ClassVar[DataType] = ...
    FLOAT32: ClassVar[DataType] = ...
    UINT16: ClassVar[DataType] = ...
    UINT32: ClassVar[DataType] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class DeviceComputeKernelConfig:
    def __init__(self, *args, **kwargs) -> None: ...

class EmbeddingsType:
    __members__: ClassVar[dict] = ...  # read-only
    BINARY: ClassVar[EmbeddingsType] = ...
    GENERIC: ClassVar[EmbeddingsType] = ...
    PADDED: ClassVar[EmbeddingsType] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class FusibleActivation:
    __members__: ClassVar[dict] = ...  # read-only
    ABS: ClassVar[FusibleActivation] = ...
    ACOS: ClassVar[FusibleActivation] = ...
    ADD_UNARY: ClassVar[FusibleActivation] = ...
    ADD_UNARY_SFPU: ClassVar[FusibleActivation] = ...
    ASIN: ClassVar[FusibleActivation] = ...
    ATAN: ClassVar[FusibleActivation] = ...
    COS: ClassVar[FusibleActivation] = ...
    DIV_UNARY: ClassVar[FusibleActivation] = ...
    DIV_UNARY_SFPU: ClassVar[FusibleActivation] = ...
    ELU: ClassVar[FusibleActivation] = ...
    EQZ: ClassVar[FusibleActivation] = ...
    ERF: ClassVar[FusibleActivation] = ...
    ERFC: ClassVar[FusibleActivation] = ...
    ERFINV: ClassVar[FusibleActivation] = ...
    EXP: ClassVar[FusibleActivation] = ...
    EXP2: ClassVar[FusibleActivation] = ...
    EXPM1: ClassVar[FusibleActivation] = ...
    GELU: ClassVar[FusibleActivation] = ...
    GEZ: ClassVar[FusibleActivation] = ...
    GTZ: ClassVar[FusibleActivation] = ...
    HEAVISIDE: ClassVar[FusibleActivation] = ...
    I0: ClassVar[FusibleActivation] = ...
    IDENTITY: ClassVar[FusibleActivation] = ...
    ISFINITE: ClassVar[FusibleActivation] = ...
    ISINF: ClassVar[FusibleActivation] = ...
    ISNAN: ClassVar[FusibleActivation] = ...
    ISNEGINF: ClassVar[FusibleActivation] = ...
    ISPOSINF: ClassVar[FusibleActivation] = ...
    LEAKY_RELU: ClassVar[FusibleActivation] = ...
    LEZ: ClassVar[FusibleActivation] = ...
    LOG: ClassVar[FusibleActivation] = ...
    LOG10: ClassVar[FusibleActivation] = ...
    LOG2: ClassVar[FusibleActivation] = ...
    LOGICAL_NOT_UNARY: ClassVar[FusibleActivation] = ...
    LTZ: ClassVar[FusibleActivation] = ...
    MUL_UNARY: ClassVar[FusibleActivation] = ...
    MUL_UNARY_SFPU: ClassVar[FusibleActivation] = ...
    NEG: ClassVar[FusibleActivation] = ...
    NEZ: ClassVar[FusibleActivation] = ...
    POWER: ClassVar[FusibleActivation] = ...
    RDIV: ClassVar[FusibleActivation] = ...
    RECIP: ClassVar[FusibleActivation] = ...
    RELU: ClassVar[FusibleActivation] = ...
    RELU6: ClassVar[FusibleActivation] = ...
    RELU_MAX: ClassVar[FusibleActivation] = ...
    RELU_MIN: ClassVar[FusibleActivation] = ...
    RSQRT: ClassVar[FusibleActivation] = ...
    RSUB: ClassVar[FusibleActivation] = ...
    SIGMOID: ClassVar[FusibleActivation] = ...
    SIGN: ClassVar[FusibleActivation] = ...
    SIGNBIT: ClassVar[FusibleActivation] = ...
    SILU: ClassVar[FusibleActivation] = ...
    SIN: ClassVar[FusibleActivation] = ...
    SQRT: ClassVar[FusibleActivation] = ...
    SQUARE: ClassVar[FusibleActivation] = ...
    SUB_UNARY: ClassVar[FusibleActivation] = ...
    SUB_UNARY_SFPU: ClassVar[FusibleActivation] = ...
    TAN: ClassVar[FusibleActivation] = ...
    TANH: ClassVar[FusibleActivation] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class FusibleActivationWithParam:
    @overload
    def __init__(self, arg0: FusibleActivation) -> None: ...
    @overload
    def __init__(self, arg0: FusibleActivation, arg1: float) -> None: ...
    @overload
    def __init__(self, arg0: Tuple[FusibleActivation, float]) -> None: ...

class GrayskullComputeKernelConfig:
    math_approx_mode: bool
    math_fidelity: MathFidelity
    def __init__(self, *args, **kwargs) -> None: ...

class Layout:
    __members__: ClassVar[dict] = ...  # read-only
    ROW_MAJOR: ClassVar[Layout] = ...
    TILE: ClassVar[Layout] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class LossReductionMode:
    __members__: ClassVar[dict] = ...  # read-only
    MEAN: ClassVar[LossReductionMode] = ...
    NONE: ClassVar[LossReductionMode] = ...
    SUM: ClassVar[LossReductionMode] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class MathFidelity:
    __members__: ClassVar[dict] = ...  # read-only
    HiFi2: ClassVar[MathFidelity] = ...
    HiFi3: ClassVar[MathFidelity] = ...
    HiFi4: ClassVar[MathFidelity] = ...
    LoFi: ClassVar[MathFidelity] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class MemoryConfig:
    shard_spec: Incomplete
    def __init__(
        self, memory_layout: TensorMemoryLayout = ..., buffer_type: BufferType = ..., shard_spec=...
    ) -> None: ...
    def is_sharded(self) -> bool: ...
    def __eq__(self, arg0: MemoryConfig) -> bool: ...
    def __ne__(self, arg0: MemoryConfig) -> bool: ...
    @property
    def buffer_type(self) -> BufferType: ...
    @property
    def interleaved(self) -> bool: ...
    @property
    def memory_layout(self) -> TensorMemoryLayout: ...

class OptimizedConvBlockConfig:
    def __init__(self, *args, **kwargs) -> None: ...
    @property
    def act_block_h_ntiles(self) -> int: ...
    @property
    def act_block_w_ntiles(self) -> int: ...
    @property
    def out_subblock_h_ntiles(self) -> int: ...
    @property
    def out_subblock_w_ntiles(self) -> int: ...

class OptimizedConvParallelizationConfig:
    def __init__(self, *args, **kwargs) -> None: ...
    @property
    def grid_size(self) -> CoreCoord: ...
    @property
    def num_cores_nhw(self) -> int: ...
    @property
    def per_core_out_matrix_height_ntiles(self) -> int: ...
    @property
    def per_core_out_matrix_width_ntiles(self) -> int: ...

class ReduceOpDim:
    __members__: ClassVar[dict] = ...  # read-only
    H: ClassVar[ReduceOpDim] = ...
    HW: ClassVar[ReduceOpDim] = ...
    W: ClassVar[ReduceOpDim] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class ReduceOpMath:
    __members__: ClassVar[dict] = ...  # read-only
    MAX: ClassVar[ReduceOpMath] = ...
    MIN: ClassVar[ReduceOpMath] = ...
    SUM: ClassVar[ReduceOpMath] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class Shape:
    @overload
    def __init__(self, arg0: List[int[4]]) -> None: ...
    @overload
    def __init__(self, shape: List[int], padded_shape: List[int] | None = ...) -> None: ...
    def without_padding(self) -> Shape: ...
    @overload
    def __eq__(self, arg0: Shape) -> bool: ...
    @overload
    def __eq__(self, arg0: List[int]) -> bool: ...
    @overload
    def __eq__(self, arg0: List[int[4]]) -> bool: ...
    @overload
    def __eq__(self, arg0: None) -> bool: ...
    @overload
    def __getitem__(self, arg0: int) -> int: ...
    @overload
    def __getitem__(self, arg0: slice) -> Shape: ...
    def __iter__(self) -> Iterator: ...
    def __len__(self) -> int: ...

class ShardOrientation:
    __members__: ClassVar[dict] = ...  # read-only
    COL_MAJOR: ClassVar[ShardOrientation] = ...
    ROW_MAJOR: ClassVar[ShardOrientation] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class ShardSpec:
    grid: CoreRangeSet
    orientation: ShardOrientation
    shape: List[int[2]]
    def __init__(self, arg0: CoreRangeSet, arg1: List[int[2]], arg2: ShardOrientation, arg3: bool) -> None: ...
    def num_cores(self) -> int: ...
    def __eq__(self, arg0: ShardSpec) -> bool: ...
    def __ne__(self, arg0: ShardSpec) -> bool: ...

class StorageType:
    __members__: ClassVar[dict] = ...  # read-only
    BORROWED: ClassVar[StorageType] = ...
    DEVICE: ClassVar[StorageType] = ...
    MULTI_DEVICE: ClassVar[StorageType] = ...
    MULTI_DEVICE_HOST: ClassVar[StorageType] = ...
    OWNED: ClassVar[StorageType] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class Tensor:
    @overload
    def __init__(self, arg0: Tensor) -> None: ...
    @overload
    def __init__(self, arg0: List[float], arg1: List[int[4]], arg2: DataType, arg3: Layout) -> None: ...
    @overload
    def __init__(
        self, arg0: List[float], arg1: List[int[4]], arg2: DataType, arg3: Layout, arg4: tt_lib.device.Device
    ) -> None: ...
    @overload
    def __init__(
        self,
        arg0: List[float],
        arg1: List[int[4]],
        arg2: DataType,
        arg3: Layout,
        arg4: tt_lib.device.Device,
        arg5: MemoryConfig,
    ) -> None: ...
    @overload
    def __init__(self, tensor: object, data_type: DataType | None = ...) -> None: ...
    @overload
    def __init__(
        self,
        tensor: object,
        data_type: DataType | None = ...,
        device: tt_lib.device.Device,
        layout: Layout,
        mem_config: MemoryConfig,
    ) -> None: ...
    @overload
    def buffer(
        self,
    ) -> (
        owned_buffer_for_uint16_t
        | owned_buffer_for_uint32_t
        | owned_buffer_for_float32_t
        | owned_buffer_for_bfloat16_t
        | borrowed_buffer_for_uint16_t
        | borrowed_buffer_for_uint32_t
        | borrowed_buffer_for_float32_t
        | borrowed_buffer_for_bfloat16_t
    ): ...
    @overload
    def buffer(self) -> Any: ...
    @overload
    def cpu(self, blocking: bool = ...) -> Tensor: ...
    @overload
    def cpu(self) -> Any: ...
    @overload
    def cpu_sharded(self) -> Tensor: ...
    @overload
    def cpu_sharded(self) -> Any: ...
    def deallocate(self, force: bool = ...) -> None: ...
    @overload
    def device(self) -> tt_lib.device.Device: ...
    @overload
    def device(self) -> Any: ...
    @overload
    def extract_shard(self, core: CoreCoord) -> Tensor: ...
    @overload
    def extract_shard(self, core_id: int) -> Tensor: ...
    @overload
    def get_dtype(self) -> DataType: ...
    @overload
    def get_dtype(self) -> Any: ...
    @overload
    def get_layout(self) -> Layout: ...
    @overload
    def get_layout(self) -> Any: ...
    @overload
    def get_legacy_shape(self) -> Shape: ...
    @overload
    def get_legacy_shape(self) -> Any: ...
    def is_allocated(self) -> bool: ...
    def is_contiguous(self) -> bool: ...
    @overload
    def is_sharded(self) -> bool: ...
    @overload
    def is_sharded(self) -> Any: ...
    @overload
    def memory_config(self) -> MemoryConfig: ...
    @overload
    def memory_config(self) -> Any: ...
    @overload
    def pad(self, arg0: List[int[4]], arg1: List[int[4]], arg2: float) -> Tensor: ...
    @overload
    def pad(self, output_tensor_shape, input_tensor_start, pad_value) -> Any: ...
    @overload
    def pad_to_tile(self, arg0: float) -> Tensor: ...
    @overload
    def pad_to_tile(self, pad_value) -> Any: ...
    @overload
    def reshape(self, arg0: int, arg1: int, arg2: int, arg3: int) -> Tensor: ...
    @overload
    def reshape(self, N, C, H, W) -> Any: ...
    @overload
    def reshape(self, arg0: Shape) -> Tensor: ...
    @overload
    def shape_without_padding(self) -> Shape: ...
    @overload
    def shape_without_padding(self) -> Any: ...
    @overload
    def storage_type(self) -> StorageType: ...
    @overload
    def storage_type(self) -> Any: ...
    @overload
    def to(self, arg0: tt_lib.device.Device, mem_config: MemoryConfig = ...) -> Tensor: ...
    @overload
    def to(self, tt_device) -> Any: ...
    @overload
    def to(self, arg0: Layout) -> Tensor: ...
    @overload
    def to_numpy(self) -> object: ...
    @overload
    def to_numpy(self) -> Any: ...
    @overload
    def to_torch(self) -> object: ...
    @overload
    def to_torch(self) -> Any: ...
    @overload
    def unpad(self, arg0: List[int[4]], arg1: List[int[4]]) -> Tensor: ...
    @overload
    def unpad(self, output_tensor_start, output_tensor_end) -> Any: ...
    @overload
    def unpad_from_tile(self, arg0: List[int[4]]) -> Tensor: ...
    @overload
    def unpad_from_tile(self, output_tensor_shape) -> Any: ...
    @overload
    def volume(self) -> int: ...
    @overload
    def volume(self) -> Any: ...
    @property
    def dtype(self) -> DataType: ...
    @property
    def layout(self) -> Layout: ...
    @property
    def shape(self): ...

class TensorMemoryLayout:
    __members__: ClassVar[dict] = ...  # read-only
    BLOCK_SHARDED: ClassVar[TensorMemoryLayout] = ...
    HEIGHT_SHARDED: ClassVar[TensorMemoryLayout] = ...
    INTERLEAVED: ClassVar[TensorMemoryLayout] = ...
    SINGLE_BANK: ClassVar[TensorMemoryLayout] = ...
    WIDTH_SHARDED: ClassVar[TensorMemoryLayout] = ...
    __entries: ClassVar[dict] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...

class WormholeComputeKernelConfig:
    fp32_dest_acc_en: bool
    math_approx_mode: bool
    math_fidelity: MathFidelity
    packer_l1_acc: bool
    def __init__(self, *args, **kwargs) -> None: ...

class borrowed_buffer_for_bfloat16_t:
    def __init__(self, *args, **kwargs) -> None: ...
    def __getitem__(self, arg0: int) -> bfloat16: ...
    def __iter__(self) -> Iterator: ...
    def __len__(self) -> int: ...

class borrowed_buffer_for_float32_t:
    def __init__(self, *args, **kwargs) -> None: ...
    def __getitem__(self, arg0: int) -> float: ...
    def __iter__(self) -> Iterator: ...
    def __len__(self) -> int: ...

class borrowed_buffer_for_uint16_t:
    def __init__(self, *args, **kwargs) -> None: ...
    def __getitem__(self, arg0: int) -> int: ...
    def __iter__(self) -> Iterator: ...
    def __len__(self) -> int: ...

class borrowed_buffer_for_uint32_t:
    def __init__(self, *args, **kwargs) -> None: ...
    def __getitem__(self, arg0: int) -> int: ...
    def __iter__(self) -> Iterator: ...
    def __len__(self) -> int: ...

class owned_buffer_for_bfloat16_t:
    def __init__(self, *args, **kwargs) -> None: ...
    def __getitem__(self, arg0: int) -> bfloat16: ...
    def __iter__(self) -> Iterator: ...
    def __len__(self) -> int: ...

class owned_buffer_for_float32_t:
    def __init__(self, *args, **kwargs) -> None: ...
    def __getitem__(self, arg0: int) -> float: ...
    def __iter__(self) -> Iterator: ...
    def __len__(self) -> int: ...

class owned_buffer_for_uint16_t:
    def __init__(self, *args, **kwargs) -> None: ...
    def __getitem__(self, arg0: int) -> int: ...
    def __iter__(self) -> Iterator: ...
    def __len__(self) -> int: ...

class owned_buffer_for_uint32_t:
    def __init__(self, *args, **kwargs) -> None: ...
    def __getitem__(self, arg0: int) -> int: ...
    def __iter__(self) -> Iterator: ...
    def __len__(self) -> int: ...

def abs(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def abs_bw(*args, **kwargs): ...
def acos(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def acos_bw(*args, **kwargs): ...
def acosh(input) -> Any: ...
def acosh_bw(*args, **kwargs): ...
def add(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def add1(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def add_bw(*args, **kwargs): ...
def add_layernorm(*args, **kwargs): ...
def add_rmsnorm(*args, **kwargs): ...
@overload
def add_unary(scalar: float, input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
@overload
def add_unary(input: Tensor, scalar: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def add_without_autoformat(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
    in_place: bool = ...,
) -> Tensor: ...
def addalpha(*args, **kwargs): ...
def addalpha_bw(*args, **kwargs): ...
def addcdiv(*args, **kwargs): ...
def addcdiv_bw(*args, **kwargs): ...
def addcmul(*args, **kwargs): ...
def addcmul_bw(*args, **kwargs): ...
def all_gather(
    input_tensors: List[Tensor], dim: int, num_links: int = ..., output_mem_config: MemoryConfig = ...
) -> List[Tensor]: ...
def angle(*args, **kwargs): ...
def angle_bw(*args, **kwargs): ...
def arange(*args, **kwargs): ...
def argmax(*args, **kwargs): ...
def argmin(*args, **kwargs): ...
def asin(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def asin_bw(*args, **kwargs): ...
@overload
def asinh(input) -> Any: ...
@overload
def asinh(input) -> Any: ...
def asinh_bw(*args, **kwargs): ...
def assign(*args, **kwargs): ...
def atan(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def atan2(*args, **kwargs): ...
def atan2_bw(*args, **kwargs): ...
def atan_bw(*args, **kwargs): ...
def atanh(*args, **kwargs): ...
def atanh_bw(*args, **kwargs): ...
def average_pool_2d(*args, **kwargs): ...
def bcast(
    input_a: Tensor, input_b: Tensor, math_op: BcastOpMath, dim: BcastOpDim, output_mem_config: MemoryConfig = ...
) -> Tensor: ...
def bcast_without_autoformat(
    input_a: Tensor, input_b: Tensor, math_op: BcastOpMath, dim: BcastOpDim, output_mem_config: MemoryConfig = ...
) -> Tensor: ...
def bert_large_ff1_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    fused_activation: FusibleActivationWithParam | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def bert_large_ff2_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def bert_large_fused_qkv_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def bert_large_post_softmax_bmm(
    arg0: Tensor, arg1: Tensor, output_mem_config: MemoryConfig = ..., output_dtype: DataType | None = ...
) -> Tensor: ...
def bert_large_pre_softmax_bmm(
    arg0: Tensor, arg1: Tensor, output_mem_config: MemoryConfig = ..., output_dtype: DataType | None = ...
) -> Tensor: ...
def bert_large_selfout_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def bias_gelu(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def bias_gelu_bw(*args, **kwargs): ...
def bias_gelu_unary(*args, **kwargs): ...
def bias_gelu_unary_bw(*args, **kwargs): ...
def binary_assign_bw(*args, **kwargs): ...
def binary_eq_bw(*args, **kwargs): ...
def binary_ge_bw(*args, **kwargs): ...
def binary_gt_bw(*args, **kwargs): ...
def binary_le_bw(*args, **kwargs): ...
def binary_lt_bw(*args, **kwargs): ...
def binary_ne_bw(*args, **kwargs): ...
def bmm(
    input_a: Tensor,
    input_b: Tensor,
    output_mem_config: MemoryConfig = ...,
    kernel_config: GrayskullComputeKernelConfig | WormholeComputeKernelConfig | None = ...,
    untilize_out: bool = ...,
) -> Tensor: ...
def bmm_tilize_untilize(
    arg0: Tensor,
    arg1: Tensor,
    arg2: Tensor,
    arg3: DataType,
    arg4: int,
    arg5: int,
    arg6: int,
    arg7: int,
    arg8: int,
    arg9: int,
    arg10: int,
    arg11: int,
    arg12: bool,
    arg13: bool,
    arg14: bool,
) -> Tensor: ...
def cbrt(*args, **kwargs): ...
def ceil_bw(*args, **kwargs): ...
def celu_bw(*args, **kwargs): ...
def clamp_bw(*args, **kwargs): ...
def clamp_max_bw(*args, **kwargs): ...
def clamp_min_bw(*args, **kwargs): ...
def clip(*args, **kwargs): ...
def clone(*args, **kwargs): ...
def complex_abs(*args, **kwargs): ...
def complex_add(
    input_a: complex.ComplexTensor, input_b: complex.ComplexTensor, output_mem_config: MemoryConfig = ...
) -> complex.ComplexTensor: ...
def complex_div(
    input_a: complex.ComplexTensor, input_b: complex.ComplexTensor, output_mem_config: MemoryConfig = ...
) -> complex.ComplexTensor: ...
def complex_mul(
    input_a: complex.ComplexTensor, input_b: complex.ComplexTensor, output_mem_config: MemoryConfig = ...
) -> complex.ComplexTensor: ...
def complex_recip(input: complex.ComplexTensor, output_mem_config: MemoryConfig = ...) -> complex.ComplexTensor: ...
def complex_sub(
    input_a: complex.ComplexTensor, input_b: complex.ComplexTensor, output_mem_config: MemoryConfig = ...
) -> complex.ComplexTensor: ...
def complex_tensor(real, imag) -> complex.ComplexTensor: ...
def concat(input_tensors: List[Tensor], dim: int = ..., output_mem_config: MemoryConfig = ...) -> Tensor: ...
def concat_bw(*args, **kwargs): ...
def conj(input: complex.ComplexTensor, output_mem_config: MemoryConfig = ...) -> complex.ComplexTensor: ...
def conv(*args, **kwargs): ...
def conv_with_address_map(*args, **kwargs): ...
def conv_with_fast_reader(*args, **kwargs): ...
def convert_conv_weight_tensor_to_special_padding_tiled_layout(*args, **kwargs): ...
def convert_conv_weight_tensor_to_tiled_layout(*args, **kwargs): ...
def copy(*args, **kwargs): ...
def cos(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def cos_bw(*args, **kwargs): ...
def cosh(*args, **kwargs): ...
def cosh_bw(*args, **kwargs): ...
def decorate_external_operation(function: function, function_name: str | None = ...) -> function: ...
def deg2rad(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def deg2rad_bw(*args, **kwargs): ...
def digamma(*args, **kwargs): ...
def digamma_bw(*args, **kwargs): ...
def div_bw(*args, **kwargs): ...
@overload
def div_unary(scalar: float, input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
@overload
def div_unary(input: Tensor, scalar: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def downsample(*args, **kwargs): ...
def dump_tensor(arg0: str, arg1) -> None: ...
def elu(input: Tensor, alpha: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def elu_bw(*args, **kwargs): ...
def embedding_bw(*args, **kwargs): ...
def embeddings(*args, **kwargs): ...
def empty(*args, **kwargs): ...
def eq(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def eqz(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def erf(input: Tensor, fast_and_approx: bool = ..., output_mem_config: MemoryConfig = ...) -> Tensor: ...
def erf_bw(*args, **kwargs): ...
def erfc(input: Tensor, fast_and_approx: bool = ..., output_mem_config: MemoryConfig = ...) -> Tensor: ...
def erfc_bw(*args, **kwargs): ...
def erfinv(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def erfinv_bw(*args, **kwargs): ...
def exp(input: Tensor, fast_and_approx: bool = ..., output_mem_config: MemoryConfig = ...) -> Tensor: ...
def exp2(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def exp2_bw(*args, **kwargs): ...
def exp_bw(*args, **kwargs): ...
def expm1(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def expm1_bw(*args, **kwargs): ...
def falcon_dense_4h_to_h_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
    packer_l1_acc: bool | None = ...,
) -> Tensor: ...
def falcon_dense_h_to_4h_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    fused_activation: FusibleActivationWithParam | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def falcon_fused_qkv_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def falcon_lm_head_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def falcon_selfout_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def fill_bw(*args, **kwargs): ...
def fill_cache(*args, **kwargs): ...
def fill_ones_rm(
    N: int, C: int, H: int, W: int, hOnes: int, wOnes: int, any: Tensor, output_mem_config: MemoryConfig = ...
) -> Tensor: ...
def fill_rm(
    N: int,
    C: int,
    H: int,
    W: int,
    hOnes: int,
    wOnes: int,
    any: Tensor,
    val_hi: float,
    val_lo: float,
    output_mem_config: MemoryConfig = ...,
) -> Tensor: ...
def fill_zero_bw(*args, **kwargs): ...
def fold(input: Tensor, stride_h: int, stride_w: int) -> Tensor: ...
def format_input_tensor(*args, **kwargs): ...
def format_output_tensor(*args, **kwargs): ...
def frac_bw(*args, **kwargs): ...
def full(*args, **kwargs): ...
def full_like(*args, **kwargs): ...
def fully_connected(*args, **kwargs): ...
def ge_bw(*args, **kwargs): ...
def geglu(*args, **kwargs): ...
def gelu(input: Tensor, fast_and_approx: bool = ..., output_mem_config: MemoryConfig = ...) -> Tensor: ...
def gelu_bw(*args, **kwargs): ...
def gez(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def global_max(*args, **kwargs): ...
def global_mean(*args, **kwargs): ...
def global_min(*args, **kwargs): ...
def global_sum(*args, **kwargs): ...
def glu(*args, **kwargs): ...
def groupnorm(*args, **kwargs): ...
def gt(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def gt_bw(*args, **kwargs): ...
def gte(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def gtz(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def hardshrink(*args, **kwargs): ...
def hardshrink_bw(*args, **kwargs): ...
def hardsigmoid(*args, **kwargs): ...
def hardsigmoid_bw(*args, **kwargs): ...
def hardswish(*args, **kwargs): ...
def hardswish_bw(*args, **kwargs): ...
def hardtanh(*args, **kwargs): ...
def hardtanh_bw(*args, **kwargs): ...
def heaviside(input: Tensor, value: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def hypot(*args, **kwargs): ...
def hypot_bw(*args, **kwargs): ...
def i0(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def i0_bw(*args, **kwargs): ...
def identity(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def imag(*args, **kwargs): ...
@overload
def interleaved_to_sharded(
    input: Tensor,
    grid: CoreCoord | CoreRangeSet,
    shard_shape: List[int[2]],
    shard_scheme: TensorMemoryLayout,
    shard_layout: ShardOrientation,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
@overload
def interleaved_to_sharded(
    input: Tensor, sharded_mem_config: MemoryConfig = ..., output_dtype: DataType | None = ...
) -> Tensor: ...
def is_imag(*args, **kwargs): ...
def is_real(*args, **kwargs): ...
def isclose(input_a, input_b, rtol, atol) -> Any: ...
def isfinite(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def isinf(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def isnan(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def isneginf(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def isposinf(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def lamb_optimizer(*args, **kwargs): ...
def layernorm(*args, **kwargs): ...
def ldexp(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def ldexp_bw(*args, **kwargs): ...
def le_bw(*args, **kwargs): ...
def leaky_relu(input: Tensor, slope: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def leaky_relu_bw(*args, **kwargs): ...
def lerp(*args, **kwargs): ...
def lerp_bw(*args, **kwargs): ...
def lez(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def lgamma(*args, **kwargs): ...
def lgamma_bw(*args, **kwargs): ...
def load_tensor(*args, **kwargs): ...
def log(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def log10(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def log10_bw(*args, **kwargs): ...
def log1p(*args, **kwargs): ...
def log1p_bw(*args, **kwargs): ...
def log2(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def log2_bw(*args, **kwargs): ...
def log_bw(*args, **kwargs): ...
def log_external_operation(arg0: function, *args, **kwargs) -> None: ...
def log_sigmoid(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def log_sigmoid_bw(*args, **kwargs): ...
def logaddexp(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def logaddexp2(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def logaddexp2_bw(*args, **kwargs): ...
def logaddexp_bw(*args, **kwargs): ...
def logical_and(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def logical_andi(*args, **kwargs): ...
def logical_not_unary(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def logical_noti(*args, **kwargs): ...
def logical_or(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def logical_ori(*args, **kwargs): ...
def logical_xor(*args, **kwargs): ...
def logical_xori(*args, **kwargs): ...
def logit(*args, **kwargs): ...
def logit_bw(*args, **kwargs): ...
def logiteps_bw(*args, **kwargs): ...
def lt(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def lt_bw(*args, **kwargs): ...
def lte(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def ltz(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def mac(*args, **kwargs): ...
def maeloss(*args, **kwargs): ...
def matmul(
    input_a: Tensor,
    input_b: Tensor,
    output_mem_config: MemoryConfig = ...,
    kernel_config: GrayskullComputeKernelConfig | WormholeComputeKernelConfig | None = ...,
    untilize_out: bool = ...,
) -> Tensor: ...
def max(*args, **kwargs): ...
def max_bw(*args, **kwargs): ...
def max_pool2d(*args, **kwargs): ...
def max_pool2d_v2(*args, **kwargs): ...
def maximum_bw(*args, **kwargs): ...
def mean_hw(*args, **kwargs): ...
def min(*args, **kwargs): ...
def min_bw(*args, **kwargs): ...
def mish(*args, **kwargs): ...
def moreh_matmul(*args, **kwargs): ...
def move(arg0: Tensor, output_mem_config: MemoryConfig | None = ...) -> Tensor: ...
def move_sharded(arg0: Tensor, output_mem_config: MemoryConfig | None = ...) -> Tensor: ...
def mseloss(*args, **kwargs): ...
def mul(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def mul_bw(*args, **kwargs): ...
@overload
def mul_unary(scalar: float, input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
@overload
def mul_unary(input: Tensor, scalar: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def multigammaln(*args, **kwargs): ...
def ne(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def ne_bw(*args, **kwargs): ...
def neg(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def neg_bw(*args, **kwargs): ...
def nextafter(*args, **kwargs): ...
def nez(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def nlp_concat_heads(arg0: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def nlp_create_qkv_heads(
    input: Tensor,
    input_kv: Tensor | None = ...,
    num_heads: int,
    num_kv_heads: int | None = ...,
    transpose_k_heads: bool = ...,
    output_mem_config: MemoryConfig = ...,
) -> List[Tensor]: ...
def nlp_create_qkv_heads_falcon7b(arg0: Tensor, output_mem_config: MemoryConfig = ...) -> List[Tensor]: ...
def normalize_global(*args, **kwargs): ...
def normalize_hw(*args, **kwargs): ...
def num_cores_to_corerange_set(arg0: int, arg1: CoreCoord, arg2: bool) -> Set[CoreRange]: ...
def ones(*args, **kwargs): ...
def ones_like(*args, **kwargs): ...
def optimized_conv(*args, **kwargs): ...
def outer(*args, **kwargs): ...
def pad(
    input: Tensor,
    output_tensor_shape: Shape,
    input_tensor_start: Shape,
    pad_value: float,
    output_mem_config: MemoryConfig = ...,
    use_multicore: bool = ...,
) -> Tensor: ...
def pad_to_tile_shape(arg0: List[int[4]], arg1: bool, arg2: bool, arg3: bool, arg4: bool) -> Shape: ...
def permute(input: Tensor, dims: List[int], output_mem_config: MemoryConfig = ...) -> Tensor: ...
def polar(*args, **kwargs): ...
def polygamma(*args, **kwargs): ...
def polygamma_bw(*args, **kwargs): ...
def polyval(*args, **kwargs): ...
def pow(*args, **kwargs): ...
def prelu(input: Tensor, weight: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def rad2deg(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def rad2deg_bw(*args, **kwargs): ...
def rdiv(input: Tensor, denominator: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def rdiv_bw(*args, **kwargs): ...
def real(*args, **kwargs): ...
def recip(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def reciprocal_bw(*args, **kwargs): ...
def reduce(
    input: Tensor,
    math_op: ReduceOpMath,
    dim: ReduceOpDim,
    scaler: float,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def reglu(*args, **kwargs): ...
def relu(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def relu6(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def relu6_bw(*args, **kwargs): ...
def relu_bw(*args, **kwargs): ...
def relu_max(input: Tensor, upper_limit: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def relu_min(input: Tensor, lower_limit: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def relu_without_autoformat(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def repeat(*args, **kwargs): ...
def repeat_interleave(*args, **kwargs): ...
def reshape(input: Tensor, W: int, Z: int, Y: int, X: int, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def reshard(input: Tensor, output_mem_config: MemoryConfig) -> Tensor: ...
def resnet_matmul(
    arg0: Tensor,
    arg1: Tensor,
    bias: Tensor | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
    math_fidelity: MathFidelity = ...,
) -> Tensor: ...
def rmsnorm(*args, **kwargs): ...
def rotary_embedding(*args, **kwargs): ...
def rotate_half(*args, **kwargs): ...
def rpow(*args, **kwargs): ...
def rpow_bw(*args, **kwargs): ...
def rsqrt(input: Tensor, fast_and_approx: bool = ..., output_mem_config: MemoryConfig = ...) -> Tensor: ...
def rsqrt_bw(*args, **kwargs): ...
def rsub(input: Tensor, value: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def rsub_bw(*args, **kwargs): ...
def scale_mask_softmax(
    input: Tensor,
    scale: float | None,
    mask: Tensor | None,
    output_mem_config: MemoryConfig = ...,
    is_causal_mask: bool = ...,
) -> Tensor: ...
def scatter(*args, **kwargs): ...
def selu_bw(*args, **kwargs): ...
def sfpu_eps(*args, **kwargs): ...
def sharded_to_interleaved(
    input: Tensor, output_mem_config: MemoryConfig = ..., output_dtype: DataType | None = ...
) -> Tensor: ...
def sigmoid(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def sigmoid_bw(*args, **kwargs): ...
def sign(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def sign_bw(*args, **kwargs): ...
def signbit(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def silu(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def silu_bw(*args, **kwargs): ...
def sin(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def sin_bw(*args, **kwargs): ...
def sinh(*args, **kwargs): ...
def sinh_bw(*args, **kwargs): ...
def softmax(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def softplus(*args, **kwargs): ...
def softplus_bw(*args, **kwargs): ...
def softshrink(*args, **kwargs): ...
def softshrink_bw(*args, **kwargs): ...
def softsign(*args, **kwargs): ...
def softsign_bw(*args, **kwargs): ...
def split_dim_two_chunks_tiled(*args, **kwargs): ...
def split_last_dim_two_chunks_tiled(*args, **kwargs): ...
def sqrt(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def sqrt_bw(*args, **kwargs): ...
def square(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def square_bw(*args, **kwargs): ...
def squared_difference(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def squared_difference_bw(*args, **kwargs): ...
def std_hw(*args, **kwargs): ...
def sub(
    input_a: Tensor,
    input_b: Tensor,
    fused_activations: List[FusibleActivationWithParam] | None = ...,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def sub_bw(*args, **kwargs): ...
@overload
def sub_unary(scalar: float, input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
@overload
def sub_unary(input: Tensor, scalar: float, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def subalpha(*args, **kwargs): ...
def subalpha_bw(*args, **kwargs): ...
def sum(*args, **kwargs): ...
def swiglu(*args, **kwargs): ...
def swish(*args, **kwargs): ...
def tan(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def tan_bw(*args, **kwargs): ...
def tanh(input: Tensor, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def tanh_bw(*args, **kwargs): ...
def tanhshrink(x) -> Any: ...
def tanhshrink_bw(*args, **kwargs): ...
def threshold(*args, **kwargs): ...
def threshold_bw(*args, **kwargs): ...
def tilize(
    input: Tensor, output_mem_config: MemoryConfig = ..., output_dtype: DataType | None = ..., use_multicore: bool = ...
) -> Tensor: ...
def tilize_with_val_padding(
    input: Tensor,
    output_tensor_shape: Shape,
    input_tensor_start: Shape,
    pad_value: float,
    output_mem_config: MemoryConfig = ...,
    output_dtype: DataType | None = ...,
) -> Tensor: ...
def tilize_with_zero_padding(
    input: Tensor, output_mem_config: MemoryConfig = ..., output_dtype: DataType | None = ...
) -> Tensor: ...
def transpose(input: Tensor, dim0: int, dim1: int, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def tril(*args, **kwargs): ...
def triu(*args, **kwargs): ...
def trunc_bw(*args, **kwargs): ...
def typecast(input_tensors: Tensor, dtype: DataType, output_mem_config: MemoryConfig = ...) -> Tensor: ...
def unary_add_bw(*args, **kwargs): ...
def unary_assign_bw(*args, **kwargs): ...
def unary_chain(
    input: Tensor, unary_chain: List[FusibleActivationWithParam], output_mem_config: MemoryConfig = ...
) -> Tensor: ...
def unary_div_bw(*args, **kwargs): ...
def unary_eq_bw(*args, **kwargs): ...
def unary_mul_bw(*args, **kwargs): ...
def unary_pow_bw(*args, **kwargs): ...
def unary_sub_bw(*args, **kwargs): ...
def unpad(
    input: Tensor, output_tensor_start: Shape, output_tensor_end: Shape, output_mem_config: MemoryConfig = ...
) -> Tensor: ...
def untilize(
    input: Tensor, output_mem_config: MemoryConfig = ..., use_multicore: bool = ..., use_pack_untilize: bool = ...
) -> Tensor: ...
def untilize_with_halo(
    input: Tensor,
    pad_val: int,
    in_b: int,
    in_h: int,
    in_w: int,
    stride: int = ...,
    output_mem_config: MemoryConfig = ...,
) -> Tensor: ...
def untilize_with_halo_v2(
    input_tensor: Tensor,
    padding_config: Tensor,
    local_config: Tensor,
    remote_config: Tensor,
    pad_val: int,
    ncores_height: int,
    max_out_nsticks_per_core: int,
    output_mem_config: MemoryConfig = ...,
) -> Tensor: ...
def untilize_with_unpadding(
    input: Tensor,
    output_tensor_start: Shape,
    output_tensor_end: Shape,
    output_mem_config: MemoryConfig = ...,
    use_pack_untilize: bool = ...,
) -> Tensor: ...
def update_cache(*args, **kwargs): ...
def upsample(*args, **kwargs): ...
def var_hw(*args, **kwargs): ...
@overload
def where(predicate, true_value, false_value) -> Any: ...
@overload
def where(predicate, true_value, false_value) -> Any: ...
@overload
def where(predicate, true_value, false_value) -> Any: ...
@overload
def where(predicate, true_value, false_value) -> Any: ...
def where_bw(*args, **kwargs): ...
def xlogy(*args, **kwargs): ...
def xlogy_bw(*args, **kwargs): ...
def zeros(*args, **kwargs): ...
def zeros_like(*args, **kwargs): ...
